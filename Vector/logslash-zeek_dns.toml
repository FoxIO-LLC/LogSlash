# Copyright (c) 2023, FoxIO, LLC.
# All rights reserved.
# US Patent No. 10,877,972
# Licenced under the FoxIO License 1.0
# For full license text, see LICENSE file in the repo root or https://github.com/FoxIO-LLC/LogSlash/blob/main/LICENSE

[sources.in_dns]
# Define your source here. https://vector.dev/docs/reference/configuration/sources/
type = "file"
include = [ "/usr/local/zeek/spool/zeek/json_streaming_dns.log" ]
read_from = "beginning"
ignore_checkpoints = true

[transforms.pipe1_dns]
type = "remap"
inputs = ["in_dns"]
source = '''
# Parsing for JSON Logs
        . = parse_json!(.message)
# Cleaning up messages
        del(.message)
        .id.orig_h = ."id.orig_h"
        .id.resp_h = ."id.resp_h"
        .id.resp_p = ."id.resp_p"
        .id.orig_p = ."id.orig_p"
# Converting start from epoch to timestamp
        .timestamp = parse_timestamp!(.ts, format: "%Y-%m-%dT%H:%M:%S%.fZ")
        # .timestamp_end = .timestamp
# Setting our LogSlash timewindow to every minute on the minute based on the timestamp
        .timewindow, err = format_timestamp(.timestamp, format: "%F%R")
# Setting our LogSlash counter
        .logslash = 1
# Cleaning up epoch times
        del(.ts)
'''

[transforms.pipe2_dns]
# Filtering out empty logs
type = "filter"
inputs = ["pipe1_dns"]
condition.type = "vrl"
condition.source =  """
        .uid != null
"""

[transforms.pipe3_dns]
type = "reduce"
inputs = ["pipe2_dns"]
# Setting "expire_after_ms = 10" can sometimes vastly improve performance against large flat files.
# For streaming inputs, you'll want to use "expire_after_ms = 60000" which will hold events in memory for 1 minute.
# expire_after_ms = 10
# Defining the fields to deduplicate by, "timewindow" is required for LogSlash to function
group_by = [ "timewindow", "id.orig_h", "id.resp_h", "id.resp_p", "proto" ]
# Performing transforms on the data to retain log value while reducing logs

# Discarding all timestamps except for the first timestamp
merge_strategies.timestamp = "discard"
# Discarding all end timestamps except for the last timestamp
merge_strategies.timestamp_end = "retain"
merge_strategies.uid = "discard"
# Discarding all srcport except for the first srcport
merge_strategies."id.orig_p" = "discard"
merge_strategies."id.resp_p" = "flat_unique"
merge_strategies.proto = "flat_unique"
merge_strategies.trans_id = "flat_unique"
merge_strategies.rtt = "sum"
merge_strategies.query = "flat_unique"
merge_strategies.qclass = "discard"
merge_strategies.qclass_name = "discard"
merge_strategies.qtype = "flat_unique"
merge_strategies.qtype_name = "flat_unique"
merge_strategies.rcode = "flat_unique"
merge_strategies.rcode_name = "discard"
merge_strategies.AA = "flat_unique"
merge_strategies.TC = "flat_unique"
merge_strategies.RD = "flat_unique"
merge_strategies.RA = "flat_unique"
merge_strategies.Z = "discard"
merge_strategies.answers = "flat_unique"
merge_strategies.TTLs = "flat_unique"
merge_strategies.rejected = "flat_unique"
# Counting how many logs have been reduced by LogSlash
merge_strategies.logslash = "sum"

[transforms.pipe4_dns]
# Cleaning up the timewindow field
type = "remap"
inputs = ["pipe3_dns"]
source = '''
        .rtt_total = .rtt
        del(.rtt)
        del(.timewindow)
'''

[sinks.out_dns]
# Define your output here. https://vector.dev/docs/reference/configuration/sinks/
inputs = ["pipe4_dns"]
type = "console"
encoding.codec = "json"
