# Copyright (c) 2023, FoxIO, LLC
# All rights reserved.
# US Patent No. 10,877,972
# Licenced under the FoxIO License 1.0
# For full license text, see LICENSE file in the repo root or https://github.com/FoxIO-LLC/LogSlash/blob/main/LICENSE

[sources.in]
# Define your source here. https://vector.dev/docs/reference/configuration/sources/
type = "file"
include = [ "conn.log" ]
read_from = "beginning"
ignore_checkpoints = true

[transforms.pipe1]
type = "remap"
inputs = ["in"]
source = '''
# Parsing for JSON Logs
        . = parse_json!(.message)
# Cleaning up messages
        del(.message)
        .id.orig_h = ."id.orig_h"
        .id.resp_h = ."id.resp_h"
        .id.resp_p = ."id.resp_p"
        .id.orig_p = ."id.orig_p"
# Converting start from epoch to timestamp
        .timestamp = to_timestamp!(.ts)
        .timestamp_end = .timestamp
# Setting our LogSlash timewindow to every minute on the minute based on the timestamp
        .timewindow, err = format_timestamp(.timestamp, format: "%F%R")
# Setting our LogSlash counter
        .logslash = 1
# Cleaning up epoch times
        del(.ts)
'''

[transforms.pipe2]
# Filtering out empty logs
type = "filter"
inputs = ["pipe1"]
condition.type = "vrl"
condition.source =  """
        .uid != null
"""

[transforms.pipe3]
type = "reduce"
inputs = ["pipe2"]
# Setting "expire_after_ms = 10" can sometimes vastly improve performance against large flat files.
# For streaming inputs, you'll want to use "expire_after_ms = 60000" which will hold events in memory for 1 minute.
# expire_after_ms = 10
# Defining the fields to deduplicate by, "timewindow" is required for LogSlash to function
group_by = [ "timewindow", "id.orig_h", "id.resp_h", "id.resp_p", "proto" ]
# Performing transforms on the data to retain log value while reducing logs
merge_strategies.duration = "sum"
merge_strategies.orig_bytes = "sum"
merge_strategies.resp_bytes = "sum"
merge_strategies.missed_bytes = "sum"
merge_strategies.orig_pkts = "sum"
merge_strategies.orig_ip_bytes = "sum"
merge_strategies.resp_pkts = "sum"
merge_strategies.resp_ip_bytes = "sum"
# Discarding all srcport except for the first srcport
merge_strategies."id.orig_p" = "discard"
merge_strategies."id.resp_p" = "flat_unique"
merge_strategies.uid = "discard"
# Discarding all timestamps except for the first timestamp
merge_strategies.timestamp = "discard"
# Discarding all end timestamps except for the last timestamp
merge_strategies.timestamp_end = "retain"
merge_strategies.proto = "flat_unique"
merge_strategies.service = "flat_unique"
merge_strategies.conn_state = "flat_unique"
# Counting how many logs have been reduced by LogSlash
merge_strategies.logslash = "sum"

[transforms.pipe4]
# Cleaning up the timewindow field
type = "remap"
inputs = ["pipe3"]
source = '''
# Renaming sum merge strategies fields
        .duration_total = .duration
        .orig_bytes_total = .orig_bytes
        .resp_bytes_total = .resp_bytes
        .missed_bytes_total = .missed_bytes
        .orig_pkts_total = .orig_pkts
        .orig_ip_bytes_total = .orig_ip_bytes
        .resp_pkts_total = .resp_pkts
        .resp_ip_bytes_total = .resp_ip_bytes
# Deleting old field names
        del(.duration)
        del(.orig_bytes)
        del(.resp_bytes)
        del(.missed_bytes)
        del(.orig_pkts)
        del(.orig_ip_bytes)
        del(.resp_pkts)
        del(.resp_ip_bytes)
        del(.timewindow)
'''

[sinks.out]
# Define your output here. https://vector.dev/docs/reference/configuration/sinks/
inputs = ["pipe4"]
type = "console"
encoding.codec = "json"
