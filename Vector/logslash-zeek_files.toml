# Copyright (c) 2023, FoxIO, LLC.
# All rights reserved.
# US Patent No. 10,877,972
# Licenced under the FoxIO License 1.0
# For full license text, see LICENSE file in the repo root or https://github.com/FoxIO-LLC/LogSlash/blob/main/LICENSE

[sources.in_files]
# Define your source here. https://vector.dev/docs/reference/configuration/sources/
type = "file"
include = [ "/usr/local/zeek/spool/zeek/json_streaming_files.log" ]
read_from = "beginning"
ignore_checkpoints = true

[transforms.pipe1_files]
type = "remap"
inputs = ["in_files"]
source = '''
# Parsing for JSON Logs
        . = parse_json!(.message)
# Cleaning up messages
        del(.message)
        .tx_hosts = ."tx_hosts"
        .rx_hosts = ."rx_hosts"
# Converting start from epoch to timestamp
        .timestamp = parse_timestamp!(.ts, format: "%Y-%m-%dT%H:%M:%S%.fZ")
        .timestamp_end = .timestamp
# Setting our LogSlash timewindow to every minute on the minute based on the timestamp
        .timewindow, err = format_timestamp(.timestamp, format: "%F%R")
# Setting our LogSlash counter
        .logslash = 1
# Cleaning up epoch times
        del(.ts)
'''

[transforms.pipe2_files]
# Filtering out empty logs
type = "filter"
inputs = ["pipe1_files"]
condition.type = "vrl"
condition.source =  """
        .fuid != null
"""

[transforms.pipe3_files]
type = "reduce"
inputs = ["pipe2_files"]
# Setting "expire_after_ms = 10" can sometimes vastly improve performance against large flat files.
# For streaming inputs, you'll want to use "expire_after_ms = 60000" which will hold events in memory for 1 minute.
# expire_after_ms = 10
# Defining the fields to deduplicate by, "timewindow" is required for LogSlash to function
group_by = [ "timewindow", "tx_hosts", "rx_hosts", "source"]

merge_strategies.timestamp = "discard"
merge_strategies.timestamp_end = "retain"
merge_strategies.fuid = "flat_unique"
merge_strategies.conn_uids = "flat_unique"
merge_strategies.source = "flat_unique"
merge_strategies.depth = "discard"
merge_strategies.analyzers = "flat_unique"
merge_strategies.duration = "flat_unique"
merge_strategies.is_orig = "flat_unique"
merge_strategies.seen_bytes = "sum"
merge_strategies.total_bytes = "sum"
merge_strategies.missing_bytes = "sum"
merge_strategies.overflow_bytes = "sum"
merge_strategies.timedout = "flat_unique"
merge_strategies.logslash = "sum"


[transforms.pipe4_files]
# Cleaning up the timewindow field
type = "remap"
inputs = ["pipe3_files"]
source = '''
# Renaming sum merge strategies fields
        .seen_bytes_total = .seen_bytes
        .missing_bytes_total = .missing_bytes
        .overflow_bytes_total = .overflow_bytes
# Deleting old field names
        del(.timewindow)
        del(.seen_bytes)
        del(.missing_bytes)
        del(.overflow_bytes)
'''

[sinks.out_files]
# Define your output here. https://vector.dev/docs/reference/configuration/sinks/
inputs = ["pipe4_files"]
type = "console"
encoding.codec = "json"
