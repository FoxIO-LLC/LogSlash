# Copyright (c) 2023, FoxIO, LLC
# All rights reserved.
# US Patent No. 10,877,972
# Licenced under the FoxIO License 1.0
# For full license text, see LICENSE file in the repo root or https://github.com/FoxIO-LLC/LogSlash/blob/main/LICENSE

# Slash-N-Stash: This sample logstash configuration processes Zeek Conn logs with LogSlash filtering standards

# Author: Steven Hostetler (steven.hostetler@gmail.com)
# Version: 1.0 (April 2023)

input {
# Define your input here. https://www.elastic.co/guide/en/logstash/current/input-plugins.html
  file {
    path => "/tmp/conn.log"
    start_position => "beginning"
    # This variable is used in the aggregate map task_id section and only needed if you expect to LogSlash multiple different types of logs
    type => "zeek_conn"
  }
}

filter {
# Extract the input json into key value pairs
  json {
    source => "message"
    # Optional: Set target for ECS compatibility. https://www.elastic.co/guide/en/ecs/current/ecs-reference.html
    # target => ""
  }

  # Convert epoch event start timestamp
  date {
    match => [ "ts","UNIX" ]
    target => "timewindow"
  }
  # Setting our LogSlash timewindow to every minute on the minute based on the timestamp
  ruby {
    code =>
      '
        ts = event.get("timewindow")
        event.set("_timewindow", Time.at(ts.to_f).strftime("%F%R"))
      '
  }
}

# Drop input logs without a valid UID
filter {
  if [uid] == "" {
    drop {}
  }
}

# Begin LogSlash filtering
# Define further aggregation here. https://www.elastic.co/guide/en/logstash/current/plugins-filters-aggregate.html
filter {
  aggregate {
    id => "logslash_aggregate_map"
    # Set unique identifier for aggregate hash
    task_id => "%{type}_%{_timewindow}_%{id.orig_h}_%{id.resp_h}_%{id.resp_p}_%{proto}"
    code =>
    "
      # Count number of events in map
      map['logslash'] ||= 0
      map['logslash'] += 1
      
      # Store events as hash values
      map['logslash_events'] ||= {}
      map['logslash_events'][map['logslash']] ||= JSON.parse(event.get('message'))
      
      # Drop initial (non-aggregate) input events
      event.cancel()
    "
    # Store hashed events in memory for a defined period of time (seconds)
    # For streaming inputs, you'll want to use "timeout => 60" which will hold events in memory for 1 minute.
    # Note: Running against large flat files may require a shorter timeout

    # The amount of seconds (since the first event) after which a task is considered as expired.
    # Compute timeout based on the timestamp in event log vs. system time (default)
    timeout_timestamp_field => "timewindow"
    timeout => 60

    # Each time a task timeout is detected, it pushes task aggregation map as a new Logstash event
    push_map_as_event_on_timeout => true

    # Fields for identifying newly generated events
    # timeout_task_id_field => "task_id"
    # timeout_tags => ['_logslash_aggregate_summary']
    # Execute after successful timeout - Summarize fields in newly generated event map
    timeout_code =>
    "
      # Pull events from aggregation map
      map = {}
      map['logslash_events'] ||= event.get('logslash_events')

      # Find first and last timestamp
      ts_values = map['logslash_events'].map { |_, event| event['ts'] }
      map['min_ts'] = ts_values.min
      map['max_ts'] = ts_values.max
      
      # Find index of first event. Order of events into the map are not guaranteed when using multiple logstash workers.
      first_event_index = ts_values.index(ts_values.min)

      # Summarize fields - exclude null values
      duration_values = map['logslash_events'].map { |_, event| event['duration'] }
      map['sum_map_duration'] ||= duration_values.compact.sum

      orig_bytes_values = map['logslash_events'].map { |_, event| event['orig_bytes'] }
      map['sum_map_orig_bytes'] ||= orig_bytes_values.compact.sum

      resp_bytes_values = map['logslash_events'].map { |_, event| event['resp_bytes'] }
      map['sum_map_resp_bytes'] ||= resp_bytes_values.compact.sum

      missed_bytes_values = map['logslash_events'].map { |_, event| event['missed_bytes'] }
      map['sum_map_missed_bytes'] ||= missed_bytes_values.compact.sum

      orig_pkts_values = map['logslash_events'].map { |_, event| event['orig_pkts'] }
      map['sum_map_orig_pkts'] ||= orig_pkts_values.compact.sum

      orig_ip_bytes_values = map['logslash_events'].map { |_, event| event['orig_ip_bytes'] }
      map['sum_map_orig_ip_bytes'] ||= orig_ip_bytes_values.compact.sum

      resp_pkts_values = map['logslash_events'].map { |_, event| event['resp_pkts'] }
      map['sum_map_resp_pkts'] ||= resp_pkts_values.compact.sum

      resp_ip_bytes_values = map['logslash_events'].map { |_, event| event['resp_ip_bytes'] }
      map['sum_map_resp_ip_bytes'] ||= resp_ip_bytes_values.compact.sum

      # Flatten values
      uid_values = map['logslash_events'].map { |_, event| event['uid'] }
      proto_values = map['logslash_events'].map { |_, event| event['proto'] }
      service_values = map['logslash_events'].map { |_, event| event['service'] }
      conn_state_values = map['logslash_events'].map { |_, event| event['conn_state'] }
      id_orig_h_values = map['logslash_events'].map { |_, event| event['id.orig_h'] }
      id_resp_h_values = map['logslash_events'].map { |_, event| event['id.resp_h'] }
      id_orig_p_values = map['logslash_events'].map { |_, event| event['id.orig_p'] }
      id_resp_p_values = map['logslash_events'].map { |_, event| event['id.resp_p'] }
      history_values = map['logslash_events'].map { |_, event| event['history'] }
      local_orig_values = map['logslash_events'].map { |_, event| event['local_orig'] }
      local_resp_values = map['logslash_events'].map { |_, event| event['local_resp'] }
   
      # Add new calculated fields and values to event map output
      event.set('[duration]', map['sum_map_duration'])
      event.set('[orig_bytes]', map['sum_map_orig_bytes'])
      event.set('[resp_bytes]', map['sum_map_resp_bytes'])
      event.set('[missed_bytes]', map['sum_map_missed_bytes'])
      event.set('[orig_pkts]', map['sum_map_orig_pkts'])
      event.set('[orig_ip_bytes]', map['sum_map_orig_ip_bytes'])
      event.set('[resp_pkts]', map['sum_map_resp_pkts'])
      event.set('[resp_ip_bytes]', map['sum_map_resp_ip_bytes'])
      event.set('[ts_start]', map['min_ts'])
      event.set('[ts_end]', map['max_ts'])

      # Add existing fields and values to event map output
      event.set('[uid]', uid_values[first_event_index])
      event.set('[proto]', proto_values[0])
      event.set('[service]', service_values.uniq)
      event.set('[conn_state]', conn_state_values.uniq)
      event.set('[id][orig_h]', id_orig_h_values[0])
      event.set('[id][resp_h]', id_resp_h_values[0])
      event.set('[id][orig_p]', id_orig_p_values[first_event_index])
      event.set('[id][resp_p]', id_resp_p_values[0])
      event.set('[history]', history_values.uniq)
      event.set('[local_orig]', local_orig_values[0])
      event.set('[local_resp]', local_resp_values[0])
    "
  }
}

# Cleanup new events generated by the aggregate map
filter {
  mutate {
    remove_field => ["@version"]
    remove_field => ["@timestamp"]
    remove_field => ["logslash_events"]
  }
  # Convert epoch timestamps
  date {
    match => [ "[ts_start]","UNIX" ]
    target => "[timestamp]"
    remove_field => [ "[ts_start]" ]
  }
  date {
    match => [ "[ts_end]","UNIX" ]
    target => "[timestamp_end]"
    remove_field => [ "[ts_end]" ]
  }
}

output {
# Define your output here. https://www.elastic.co/guide/en/logstash/current/output-plugins.html
  # Send new aggregate events generated to a dedicated log file  
  file {
    id => "logslash_conn_output"
    path => "/tmp/slashed.conn.log"
    codec => "json_lines"
  }
}
